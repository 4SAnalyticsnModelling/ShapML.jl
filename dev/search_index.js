var documenterSearchIndex = {"docs":
[{"location":"vignettes/consistency/#Purpose-1","page":"Stochastic vs. TreeSHAP","title":"Purpose","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"The goal of this vignette is to demonstrate how, for the same   boosted tree prediction model, the stochastic Shapley values from   ShapML correlate with the non-stochastic, tree-based Shapley   values from the Python shap   package using the implementation discussed   here.\nWhile shap provides the preferred Shapley value algorithm when   modeling with boosted trees, this vignette demonstrates that the   sampling-based ShapML implementation returns nearly identical   results while having the ability to work with all classes of ML   models.","category":"page"},{"location":"vignettes/consistency/#Setup-1","page":"Stochastic vs. TreeSHAP","title":"Setup","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"Because the tree-based Shapley value algorithm is not currently   available in Julia, we’ll use   catboost’s R package which provides a   port of the tree-based algorithm in shap in   catboost.get_feature_importance().\nOutline of the comparison:\nR: Train the ML model.\nR: Calculate the tree-based Shapley values.\nR: Write the predict() wrapper that works with the trained  model.\nJulia: Using RCall, convert the trained model and  predict() function into Julia objects.\nJulia: Calculate the stochastic Shapley values, passing in the  objects from step 4.\nR: Compare the results.","category":"page"},{"location":"vignettes/consistency/#Comparison-1","page":"Stochastic vs. TreeSHAP","title":"Comparison","text":"","category":"section"},{"location":"vignettes/consistency/#Load-Packages-1","page":"Stochastic vs. TreeSHAP","title":"Load Packages","text":"","category":"section"},{"location":"vignettes/consistency/#R-1","page":"Stochastic vs. TreeSHAP","title":"R","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"Allow rmarkdown to pass Julia and R objects between code   blocks.","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"library(JuliaCall)\n\nJuliaCall::julia_setup()\n\n# Setting the Julia path first in the R environment points JuliaCall to the right Julia .dll files.\nSys.setenv(PATH = paste(\"C:/Users/nredell/AppData/Local/Julia-1.3.1/bin\", Sys.getenv(\"PATH\"), sep = \";\"))\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(shapFlex)\nlibrary(devtools)\n\nif (!\"catboost\" %in% installed.packages()[, \"Package\"]) {\n  # Install catboost which is not available on CRAN (Windows link below).\n  devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.20/catboost-R-Windows-0.20.tgz',\n                        INSTALL_opts = c(\"--no-multiarch\"))\n}\n\nlibrary(catboost)  # version 0.20","category":"page"},{"location":"vignettes/consistency/#Julia-1","page":"Stochastic vs. TreeSHAP","title":"Julia","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"using ShapML\nusing Random\nusing DataFrames\nusing RCall","category":"page"},{"location":"vignettes/consistency/#Load-Data-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Load Data in R","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data(\"data_adult\", package = \"shapFlex\")\ndata <- data_adult\n\noutcome_name <- \"income\"  # A binary outcome.\noutcome_col <- which(names(data) == outcome_name)","category":"page"},{"location":"vignettes/consistency/#Train-ML-Model-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Train ML Model in R","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"The accuracy of the model isn’t entirely important because we’re   interested in comparing Shapley values across algorithms: stochastic   in Julia vs. tree-based in R.","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"cat_features <- which(unlist(Map(is.factor, data[, -outcome_col]))) - 1\n\ndata_pool <- catboost.load_pool(data = data[, -outcome_col],\n                                label = as.vector(as.numeric(data[, outcome_col])) - 1,\n                                cat_features = cat_features)\n\nset.seed(224)\nmodel_catboost <- catboost.train(data_pool, NULL,\n                                 params = list(loss_function = 'CrossEntropy',\n                                               iterations = 30, logging_level = \"Silent\"))","category":"page"},{"location":"vignettes/consistency/#Shapley-Algorithms-1","page":"Stochastic vs. TreeSHAP","title":"Shapley Algorithms","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"We’ll explain the same 300 instances with each algorithm.","category":"page"},{"location":"vignettes/consistency/#Tree-based-Shapley-values-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Tree-based Shapley values in R","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_pool <- catboost.load_pool(data = data[1:300, -outcome_col],\n                                label = as.vector(as.numeric(data[1:300, outcome_col])) - 1,\n                                cat_features = cat_features)\n\ndata_shap_tree <- catboost.get_feature_importance(model_catboost, pool = data_pool,\n                                                  type = \"ShapValues\")\n\ndata_shap_tree <- data.frame(data_shap_tree[, -ncol(data_shap_tree)])  # Remove the intercept column.\n\ndata_shap_tree$index <- 1:nrow(data_shap_tree)\n\ndata_shap_tree <- tidyr::gather(data_shap_tree, key = \"feature_name\",\n                                value = \"shap_effect_catboost\", -index)","category":"page"},{"location":"vignettes/consistency/#Stochastic-Shapley-values-in-Julia-1","page":"Stochastic vs. TreeSHAP","title":"Stochastic Shapley values in Julia","text":"","category":"section"},{"location":"vignettes/consistency/#Predict-function-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Predict function in R","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"For ShapML, the required user-defined prediction function takes 2   positional arguments and returns a 1-column DataFrame of model   predictions.","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"predict_function <- function(model, data) {\n\n  data_pool <- catboost.load_pool(data = data, cat_features = cat_features)\n\n  # Predictions and Shapley explanations will be in log-odds space.\n  data_pred <- data.frame(\"y_pred\" = catboost.predict(model, data_pool))\n\n  return(data_pred)\n}","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"In Julia, convert the input data, the trained model, and the   predict() function into Julia objects.","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data = RCall.reval(\"data\")\ndata = convert(DataFrame, data)\n\noutcome_name = RCall.reval(\"outcome_name\")\noutcome_name = convert(String, outcome_name)\n\nmodel_catboost = RCall.reval(\"model_catboost\")\n\npredict_function = RCall.reval(\"predict_function\")\npredict_function = convert(Function, predict_function)","category":"page"},{"location":"vignettes/consistency/#ShapML.shap-1","page":"Stochastic vs. TreeSHAP","title":"ShapML.shap","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"explain = copy(data[1:300, :])  # Compute Shapley feature-level predictions for all instances.\nexplain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nreference = copy(data)  # An optional dataset for computing the intercept/baseline prediction.\nreference = select(reference, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nRandom.seed!(224)\ndata_shap = ShapML.shap(explain = explain,\n                        reference = reference,\n                        model = model_catboost,\n                        predict_function = predict_function,\n                        sample_size = 100  # Number of Monte Carlo samples.\n                        )","category":"page"},{"location":"vignettes/consistency/#Results-1","page":"Stochastic vs. TreeSHAP","title":"Results","text":"","category":"section"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"For 10 out of 13 model features, the correlation between the   stochastic and tree-based Shapley values was >= .99 and above   .92 for the remaining features.","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_shap <- JuliaCall::julia_eval(\"data_shap\")  # Pass from Julia to R.\ndata_shap$feature_value <- NULL","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_all <- dplyr::inner_join(data_shap, data_shap_tree, by = c(\"index\", \"feature_name\"))","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_cor <- data_all %>%\n  dplyr::group_by(feature_name) %>%\n  dplyr::summarise(\"cor_coef\" = round(cor(shap_effect, shap_effect_catboost), 3))\n\ndata_cor","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"## # A tibble: 13 x 2\n##    feature_name   cor_coef\n##    <chr>             <dbl>\n##  1 age               0.994\n##  2 capital_gain      0.997\n##  3 capital_loss      0.983\n##  4 education         0.991\n##  5 education_num     0.998\n##  6 hours_per_week    0.993\n##  7 marital_status    0.99\n##  8 native_country    0.99\n##  9 occupation        0.996\n## 10 race              0.998\n## 11 relationship      0.991\n## 12 sex               0.924\n## 13 workclass         0.975","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"p <- ggplot(data_all, aes(shap_effect_catboost, shap_effect))\np <- p + geom_point(alpha = .25)\np <- p + geom_abline(color = \"red\")\np <- p + facet_wrap(~ feature_name, scales = \"free\")\np <- p + theme_bw() + xlab(\"catboost tree-based Shapley values\") + ylab(\"ShapML stochastic Shapley values\") +\n  theme(axis.title = element_text(face = \"bold\"))\np","category":"page"},{"location":"vignettes/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"(Image: )","category":"page"},{"location":"functions/functions/#","page":"Functions","title":"Functions","text":"CurrentModule = ShapML","category":"page"},{"location":"functions/functions/#","page":"Functions","title":"Functions","text":"shap","category":"page"},{"location":"functions/functions/#ShapML.shap","page":"Functions","title":"ShapML.shap","text":"shap(explain::DataFrame,\n     reference::Union{DataFrame, Nothing} = nothing,\n     model,\n     predict_function,\n     target_features::Union{Vector, Nothing} = nothing,\n     sample_size::Integer = 60,\n     parallel::Symbol = [:none, :samples],\n     seed::Integer = 1)\n\nCompute stochastic feature-level Shapley values for any ML model.\n\nArguments\n\nexplain::DataFrame: A DataFrame of model features with 1 or more instances to be explained using Shapley values.\nreference: Optional. A DataFrame with the same format as explain which serves as a reference group against which the Shapley value deviations from explain are compared (i.e., the model intercept).\nmodel: A trained ML model that is passed into predict_function.\npredict_function: A wrapper function that takes 2 required positional arguments–(1) the trained model from model and (2) a DataFrame of instances with the same format as explain. The function should return a 1-column DataFrame of model predictions; the column name does not matter.\ntarget_features: Optional. An Array{String, 1} of model features that is a subset of feature names in explain for which Shapley values will be computed. For high-dimensional models, selecting a subset of features may dramatically speed up computation time. The default behavior is to return Shapley values for all instances and features in explain.\nsample_size::Integer: The number of Monte Carlo samples used to compute the stochastic Shapley values for each feature.\nparallel::Union{Symbol, Nothing}: One of [:none, :samples]. Whether to perform the calculation serially (:none) or in parallel (:samples) with pmap().\nseed::Integer: A number passed to Random.seed!() to get reproducible results.\n\nReturn\n\nA size(explain, 1) * length(target_features) row by 6 column DataFrame.\nindex: An instance in explain.\nfeature_name: Model feature.\nfeature_value: Feature value.\nshap_effect: The average Shapley value across Monte Carlo samples.\nshap_effect_sd: The standard deviation of Shapley values across Monte Carlo samples.\nintercept: The average model prediction from explain or reference.\n\n\n\n\n\n","category":"function"},{"location":"#ShapML.jl-1","page":"Introduction","title":"ShapML.jl","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"The purpose of ShapML is to compute stochastic feature-level Shapley values which can be used to (a) interpret and/or (b) assess the fairness of any machine learning model. Shapley values are an intuitive and theoretically sound model-agnostic diagnostic tool to understand both global feature importance across all instances in a data set and instance/row-level local feature importance in black-box machine learning models.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"This package implements the algorithm described in Štrumbelj and Kononenko's (2014) sampling-based Shapley approximation algorithm to compute the stochastic Shapley values for a given instance and model feature.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Flexibility:\nShapley values can be estimated for any machine learning model using a simple user-defined predict() wrapper function.\nSpeed:\nThe speed advantage of ShapML comes in the form of giving the user the ability to select 1 or more target features of interest and avoid having to compute Shapley values for all model features (i.e., a subset of target features from a trained model will return the same feature-level Shapley values as the full model with all features). This is especially useful in high-dimensional models as the computation of a Shapley value is exponential in the number of features.","category":"page"},{"location":"#Install-1","page":"Introduction","title":"Install","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"using Pkg\nPkg.add(\"ShapML\")","category":"page"},{"location":"#Documentation-1","page":"Introduction","title":"Documentation","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"Docs","category":"page"},{"location":"#Examples-1","page":"Introduction","title":"Examples","text":"","category":"section"},{"location":"#Random-Forest-regression-model-Non-parallel-1","page":"Introduction","title":"Random Forest regression model - Non-parallel","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"We'll explain the impact of 13 features from the Boston Housing dataset on the predicted outcome MedV–or the median value of owner-occupied homes in 1000's–using predictions from a trained Random Forest regression model and stochastic Shapley values.\nWe'll explain a subset of 300 instances and then assess global feature importance by aggregating the unique feature importances for each of these instances.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using ShapML\nusing RDatasets\nusing DataFrames\nusing MLJ  # Machine learning\nusing Gadfly  # Plotting\n\n# Load data.\nboston = RDatasets.dataset(\"MASS\", \"Boston\")\n#------------------------------------------------------------------------------\n# Train a machine learning model; currently limited to single outcome regression and binary classification.\noutcome_name = \"MedV\"\n\n# Data prep.\ny, X = MLJ.unpack(boston, ==(Symbol(outcome_name)), colname -> true)\n\n# Instantiate an ML model; choose any single-outcome ML model from any package.\nrandom_forest = @load RandomForestRegressor pkg = \"DecisionTree\"\nmodel = MLJ.machine(random_forest, X, y)\n\n# Train the model.\nfit!(model)\n\n# Create a wrapper function that takes the following positional arguments: (1) a\n# trained ML model from any Julia package, (2) a DataFrame of model features. The\n# function should return a 1-column DataFrame of predictions--column names do not matter.\nfunction predict_function(model, data)\n  data_pred = DataFrame(y_pred = predict(model, data))\n  return data_pred\nend\n#------------------------------------------------------------------------------\n# ShapML setup.\nexplain = copy(boston[1:300, :]) # Compute Shapley feature-level predictions for 300 instances.\nexplain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nreference = copy(boston)  # An optional reference population to compute the baseline prediction.\nreference = select(reference, Not(Symbol(outcome_name)))\n\nsample_size = 60  # Number of Monte Carlo samples.\n#------------------------------------------------------------------------------\n# Compute stochastic Shapley values.\ndata_shap = ShapML.shap(explain = explain,\n                        reference = reference,\n                        model = model,\n                        predict_function = predict_function,\n                        sample_size = sample_size,\n                        seed = 1\n                        )\n\nshow(data_shap, allcols = true)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: shapoutput)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Now we'll create several plots that summarize the Shapley results for our Random Forest model. These plots will eventually be refined and incorporated into ShapML.\nGlobal feature importance\nBecause Shapley values represent deviations from the average or baseline prediction, plotting their average absolute value for each feature gives a sense of the magnitude with which they affect model predictions across all explained instances.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"data_plot = DataFrames.by(data_shap, [:feature_name],\n                          mean_effect = [:shap_effect] => x -> mean(abs.(x.shap_effect)))\n\ndata_plot = sort(data_plot, order(:mean_effect, rev = true))\n\nbaseline = round(data_shap.intercept[1], digits = 1)\n\np = plot(data_plot, y = :feature_name, x = :mean_effect, Coord.cartesian(yflip = true),\n         Scale.y_discrete, Geom.bar(position = :dodge, orientation = :horizontal),\n         Theme(bar_spacing = 1mm),\n         Guide.xlabel(\"|Shapley effect| (baseline = $baseline)\"), Guide.ylabel(nothing),\n         Guide.title(\"Feature Importance - Mean Absolute Shapley Value\"))\np","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: featureimportance)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Global feature effects\nThe plot below shows how changing the value of the Rm feature–the most influential feature overall–affects model predictions (holding the other features constant). Each point represents 1 of our 300 explained instances. The black line is a loess line of best fit to summarize the effect.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"data_plot = data_shap[data_shap.feature_name .== \"Rm\", :]  # Selecting 1 feature for ease of plotting.\n\nbaseline = round(data_shap.intercept[1], digits = 1)\n\np_points = layer(data_plot, x = :feature_value, y = :shap_effect, Geom.point())\np_line = layer(data_plot, x = :feature_value, y = :shap_effect, Geom.smooth(method = :loess, smoothing = 0.5),\n               style(line_width = 0.75mm,), Theme(default_color = \"black\"))\np = plot(p_line, p_points, Guide.xlabel(\"Feature value\"), Guide.ylabel(\"Shapley effect (baseline = $baseline)\"),\n         Guide.title(\"Feature Effect - $(data_plot.feature_name[1])\"))\np","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: featureeffects)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"#Random-Forest-regression-model-Parallel-1","page":"Introduction","title":"Random Forest regression model - Parallel","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"We'll explain the same dataset with the same model, but this time we'll compute the Shapley values in parallel across cores using the built-in distributed computing in ShapML which implements Distributed.pmap() internally.\nThe stochastic Shapley values will be computed in parallel over 6 cores on the same machine.\nWith the same seed set, non-parallel and parallel computation will return the same results.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using Distributed\naddprocs(6)  # 6 cores.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"The @everywhere block of code will load the relevant packages on each core. If you use another ML package, you would swap it in for using MLJ.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"@everywhere begin\n  using ShapML\n  using DataFrames\n  using MLJ\nend","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"using RDatasets\n\n# Load data.\nboston = RDatasets.dataset(\"MASS\", \"Boston\")\n#------------------------------------------------------------------------------\n# Train a machine learning model; currently limited to single outcome regression and binary classification.\noutcome_name = \"MedV\"\n\n# Data prep.\ny, X = MLJ.unpack(boston, ==(Symbol(outcome_name)), colname -> true)\n\n# Instantiate an ML model; choose any single-outcome ML model from any package.\nrandom_forest = @load RandomForestRegressor pkg = \"DecisionTree\"\nmodel = MLJ.machine(random_forest, X, y)\n\n# Train the model.\nfit!(model)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"@everywhere is needed to properly initialize the predict() wrapper function.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"# Create a wrapper function that takes the following positional arguments: (1) a\n# trained ML model from any Julia package, (2) a DataFrame of model features. The\n# function should return a 1-column DataFrame of predictions--column names do not matter.\n@everywhere function predict_function(model, data)\n  data_pred = DataFrame(y_pred = predict(model, data))\n  return data_pred\nend","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"Notice that we've set ShapML.shap(parallel = :samples) to perform the computation in parallel across our 60 Monte Carlo samples.","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"# ShapML setup.\nexplain = copy(boston[1:300, :]) # Compute Shapley feature-level predictions for 300 instances.\nexplain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nreference = copy(boston)  # An optional reference population to compute the baseline prediction.\nreference = select(reference, Not(Symbol(outcome_name)))\n\nsample_size = 60  # Number of Monte Carlo samples.\n#------------------------------------------------------------------------------\n# Compute stochastic Shapley values.\ndata_shap = ShapML.shap(explain = explain,\n                        reference = reference,\n                        model = model,\n                        predict_function = predict_function,\n                        sample_size = sample_size,\n                        parallel = :samples,  # Parallel computation over \"sample_size\".\n                        seed = 1\n                        )\n\nshow(data_shap, allcols = true)","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"(Image: shapoutput)","category":"page"}]
}
