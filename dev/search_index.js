var documenterSearchIndex = {"docs":
[{"location":"man/consistency/#Purpose-1","page":"Stochastic vs. TreeSHAP","title":"Purpose","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"The goal of this vignette is to demonstrate how, for the same   boosted tree prediction model, the stochastic Shapley values from   ShapML correlate with the non-stochastic, tree-based Shapley   values from the Python shap   package using the implementation discussed   here.\nWhile shap provides the preferred Shapley value algorithm when   modeling with boosted trees, this vignette demonstrates that the   sampling-based ShapML implementation returns nearly identical   results while having the ability to work with all classes of ML   models.","category":"page"},{"location":"man/consistency/#Setup-1","page":"Stochastic vs. TreeSHAP","title":"Setup","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"Because the tree-based Shapley value algorithm is not currently   available in Julia, we’ll use   catboost’s R package which provides a   port of the tree-based algorithm in shap in   catboost.get_feature_importance().\nOutline of the comparison:\nR: Train the ML model.\nR: Calculate the tree-based Shapley values.\nR: Write the predict() wrapper that works with the trained  model.\nJulia: Using RCall, convert the trained model and  predict() function into Julia objects.\nJulia: Calculate the stochastic Shapley values, passing in the  objects from step 4.\nR: Compare the results.","category":"page"},{"location":"man/consistency/#Example-1","page":"Stochastic vs. TreeSHAP","title":"Example","text":"","category":"section"},{"location":"man/consistency/#Load-Packages-1","page":"Stochastic vs. TreeSHAP","title":"Load Packages","text":"","category":"section"},{"location":"man/consistency/#R-1","page":"Stochastic vs. TreeSHAP","title":"R","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"Allow rmarkdown to pass Julia and R objects between code   blocks.","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"library(JuliaCall)\n\nJuliaCall::julia_setup()\n\n# Setting the Julia path first in the R environment points JuliaCall to the right Julia .dll files.\nSys.setenv(PATH = paste(\"C:/Users/nredell/AppData/Local/Julia-1.3.1/bin\", Sys.getenv(\"PATH\"), sep = \";\"))\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(shapFlex)\nlibrary(devtools)\n\nif (!\"catboost\" %in% installed.packages()[, \"Package\"]) {\n  # Install catboost which is not available on CRAN (Windows link below).\n  devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.20/catboost-R-Windows-0.20.tgz',\n                        INSTALL_opts = c(\"--no-multiarch\"))\n}\n\nlibrary(catboost)  # version 0.20","category":"page"},{"location":"man/consistency/#Julia-1","page":"Stochastic vs. TreeSHAP","title":"Julia","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"using ShapML\nusing Random\nusing DataFrames\nusing RCall","category":"page"},{"location":"man/consistency/#Load-Data-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Load Data in R","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data(\"data_adult\", package = \"shapFlex\")\ndata <- data_adult\n\noutcome_name <- \"income\"  # A binary outcome.\noutcome_col <- which(names(data) == outcome_name)","category":"page"},{"location":"man/consistency/#Train-ML-Model-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Train ML Model in R","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"The accuracy of the model isn’t entirely important because we’re   interested in comparing Shapley values across algorithms: stochastic   in Julia vs. tree-based in R.","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"cat_features <- which(unlist(Map(is.factor, data[, -outcome_col]))) - 1\n\ndata_pool <- catboost.load_pool(data = data[, -outcome_col],\n                                label = as.vector(as.numeric(data[, outcome_col])) - 1,\n                                cat_features = cat_features)\n\nset.seed(224)\nmodel_catboost <- catboost.train(data_pool, NULL,\n                                 params = list(loss_function = 'CrossEntropy',\n                                               iterations = 30, logging_level = \"Silent\"))","category":"page"},{"location":"man/consistency/#Shapley-Algorithms-1","page":"Stochastic vs. TreeSHAP","title":"Shapley Algorithms","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"We’ll explain the same 300 instances with each algorithm.","category":"page"},{"location":"man/consistency/#Tree-based-Shapley-values-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Tree-based Shapley values in R","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_pool <- catboost.load_pool(data = data[1:300, -outcome_col],\n                                label = as.vector(as.numeric(data[1:300, outcome_col])) - 1,\n                                cat_features = cat_features)\n\ndata_shap_tree <- catboost.get_feature_importance(model_catboost, pool = data_pool,\n                                                  type = \"ShapValues\")\n\ndata_shap_tree <- data.frame(data_shap_tree[, -ncol(data_shap_tree)])  # Remove the intercept column.\n\ndata_shap_tree$index <- 1:nrow(data_shap_tree)\n\ndata_shap_tree <- tidyr::gather(data_shap_tree, key = \"feature_name\",\n                                value = \"shap_effect_catboost\", -index)","category":"page"},{"location":"man/consistency/#Stochastic-Shapley-values-in-Julia-1","page":"Stochastic vs. TreeSHAP","title":"Stochastic Shapley values in Julia","text":"","category":"section"},{"location":"man/consistency/#Predict-function-in-R-1","page":"Stochastic vs. TreeSHAP","title":"Predict function in R","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"For ShapML, the required user-defined prediction function takes 2   positional arguments and returns a 1-column DataFrame of model   predictions.","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"predict_function <- function(model, data) {\n\n  data_pool <- catboost.load_pool(data = data, cat_features = cat_features)\n\n  # Predictions and Shapley explanations will be in log-odds space.\n  data_pred <- data.frame(\"y_pred\" = catboost.predict(model, data_pool))\n\n  return(data_pred)\n}","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"In Julia, convert the input data, the trained model, and the   predict() function into Julia objects.","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data = RCall.reval(\"data\")\ndata = convert(DataFrame, data)\n\noutcome_name = RCall.reval(\"outcome_name\")\noutcome_name = convert(String, outcome_name)\n\nmodel_catboost = RCall.reval(\"model_catboost\")\n\npredict_function = RCall.reval(\"predict_function\")\npredict_function = convert(Function, predict_function)","category":"page"},{"location":"man/consistency/#ShapML.shap-1","page":"Stochastic vs. TreeSHAP","title":"ShapML.shap","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"explain = copy(data[1:300, :])  # Compute Shapley feature-level predictions for all instances.\nexplain = select(explain, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nreference = copy(data)  # An optional dataset for computing the intercept/baseline prediction.\nreference = select(reference, Not(Symbol(outcome_name)))  # Remove the outcome column.\n\nRandom.seed!(224)\ndata_shap = ShapML.shap(explain = explain,\n                        reference = reference,\n                        model = model_catboost,\n                        predict_function = predict_function,\n                        sample_size = 100  # Number of Monte Carlo samples.\n                        )","category":"page"},{"location":"man/consistency/#Results-1","page":"Stochastic vs. TreeSHAP","title":"Results","text":"","category":"section"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"For 10 out of 13 model features, the correlation between the   stochastic and tree-based Shapley values was \\> .99 and above   .92 for the remaining features.","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_shap <- JuliaCall::julia_eval(\"data_shap\")  # Pass from Julia to R.\ndata_shap$feature_value <- NULL","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_all <- dplyr::inner_join(data_shap, data_shap_tree, by = c(\"index\", \"feature_name\"))","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"data_cor <- data_all %>%\n  dplyr::group_by(feature_name) %>%\n  dplyr::summarise(\"cor_coef\" = round(cor(shap_effect, shap_effect_catboost), 3))\n\ndata_cor","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"## # A tibble: 13 x 2\n##    feature_name   cor_coef\n##    <chr>             <dbl>\n##  1 age               0.994\n##  2 capital_gain      0.997\n##  3 capital_loss      0.983\n##  4 education         0.991\n##  5 education_num     0.998\n##  6 hours_per_week    0.993\n##  7 marital_status    0.99\n##  8 native_country    0.99\n##  9 occupation        0.996\n## 10 race              0.998\n## 11 relationship      0.991\n## 12 sex               0.924\n## 13 workclass         0.975","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"<br>","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"p <- ggplot(data_all, aes(shap_effect_catboost, shap_effect))\np <- p + geom_point(alpha = .25)\np <- p + geom_abline(color = \"red\")\np <- p + facet_wrap(~ feature_name, scales = \"free\")\np <- p + theme_bw() + xlab(\"catboost tree-based Shapley values\") + ylab(\"ShapML stochastic Shapley values\") +\n  theme(axis.title = element_text(face = \"bold\"))\np","category":"page"},{"location":"man/consistency/#","page":"Stochastic vs. TreeSHAP","title":"Stochastic vs. TreeSHAP","text":"<br>","category":"page"},{"location":"#ShapML.jl-Documentation-1","page":"Introduction","title":"ShapML.jl Documentation","text":"","category":"section"},{"location":"#","page":"Introduction","title":"Introduction","text":"CurrentModule = ShapML","category":"page"},{"location":"#","page":"Introduction","title":"Introduction","text":"shap","category":"page"},{"location":"#ShapML.shap","page":"Introduction","title":"ShapML.shap","text":"shap(explain::DataFrame, reference = nothing, model,\n     predict_function, target_features = nothing, sample_size::Integer = 60)\n\nCompute stochastic feature-level Shapley values for any ML model.\n\nArguments\n\nexplain::DataFrame: A DataFrame of model features with 1 or more instances to be explained using Shapley values.\nreference: Optional. A DataFrame with the same format as explain which serves as a reference group against which the Shapley value deviations from explain are compared (i.e., the model intercept).\nmodel: A trained ML model that is passed into predict_function.\npredict_function: A wrapper function that takes 2 required positional arguments–(1) the trained model from model and (2) a DataFrame of instances with the same format as explain. The function should return a 1-column DataFrame of model predictions; the column name does not matter.\ntarget_features: Optional. A character vector that is a subset of feature names in explain for which Shapley values will be computed. For high-dimensional models, selecting a subset of features may dramatically speed up computation time. The default behavior is to return Shapley values for all instances and features in explain.\nsample_size::Integer: The number of Monte Carlo samples used to compute the stochastic Shapley values for each feature.\n\nReturn\n\nA size(explain, 1) * length(target_features) row by 6 column DataFrame.\nindex: An instance in explain.\nfeature_name: Model feature.\nfeature_value: Feature value.\nshap_effect: The average Shapley value across Monte Carlo samples.\nshap_effect_sd: The standard deviation of Shapley values across Monte Carlo samples.\nintercept: The average model prediction from explain or reference.\n\n\n\n\n\n","category":"function"}]
}
